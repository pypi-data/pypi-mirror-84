# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/03_domain.model.ipynb (unless otherwise specified).


from __future__ import annotations


__all__ = ['logger', '__all__', 'HiveMixin', 'DeltaLakeMixin', 'DataSource', 'DestTable', 'PipelineCollection',
           'ModelMeta', 'ModelAbcMeta', 'AbstractModel', 'PipelineFuncStruct', 'SparkModel', 'PandasModel']

# Cell
#export
#nbdev_comment from __future__ import annotations
import abc
import logging
from pyspark.sql import DataFrame
from functools import wraps, partial
from ysyfinance import commons
import copy
import pyspark.sql.functions as F
from ysyfinance import config
from pyspark.sql import SparkSession
from typing import Any
from collections import defaultdict, namedtuple
from operator import itemgetter
from dataclasses import dataclass
import IPython.core.debugger as db
from pathlib import Path
logger = logging.getLogger(__name__)

# Cell
__all__=['AbstractModel','PipelineCollection','SparkModel','HiveMixin','DeltaLakeMixin','PandasModel', \
        'ModelMeta', 'ModelAbcMeta','DataSource', 'DestTable']

# Cell
class HiveMixin:
    def parse_dest_table(self, dest_table: str) -> (str, str) :
        #dest_table has the format: zone.database.table
        names = dest_table.split(".") \
                if dest_table is not None \
                and len(dest_table)>0 else None
        if names is None or names=='' or len(names)<3:
            raise Exception(f"Destination table {dest_table} has wrong name")
        zone_name=names[0]
        database_name=names[1]
        table_name=names[2]
        return (zone_name, database_name, table_name)

    def save(self, **kwargs):
        dest_table = kwargs.get('dest_table')
        mode = kwargs.get('mode') if kwargs.get('mode') is not None else 'overwrite'
        zone_name, database_name, table_name = self.parse_dest_table(dest_table)

        self._data.write \
                        .mode(mode) \
                        .save(f"{self.data_path}/{zone_name}/{database_name}/{table_name}")

    def saveAsTable(self, **kwargs):
        dest_table = kwargs.get('dest_table')
        mode = kwargs.get('mode') if kwargs.get('mode') is not None else 'overwrite'
        _, database_name, table_name = self.parse_dest_table(dest_table)

        self.sql(f"CREATE DATABASE IF NOT EXISTS {database_name}")
        self.sql(f"USE {database_name}")

        self._data.write \
                        .mode(mode) \
                        .saveAsTable(table_name)


# Cell
class DeltaLakeMixin(HiveMixin):
    def save(self, **kwargs):
        #db.set_trace()
        dest_table = kwargs.get('dest_table')
        mode = kwargs.get('mode') if kwargs.get('mode') is not None else 'overwrite'
        zone_name, database_name, table_name = self.parse_dest_table(dest_table)

        self._dataobject.write \
                        .format('delta') \
                        .mode(mode) \
                        .option("mergeSchema", "true") \
                        .option("overwriteSchema", "true") \
                        .save(f"{self.data_path}/{zone_name}/{database_name}/{table_name}")

    def upsert(self, **kwargs):
#         db.set_trace()
        from delta.tables import DeltaTable
        dest_table = kwargs.get('dest_table', None)
        primary_key = kwargs.get('primary_key', None)

        zone_name, database_name, table_name = self.parse_dest_table(dest_table)
        dest_table_file = f"{self.data_path}/{zone_name}/{database_name}/{table_name}"

        db.set_trace()
        if not Path(dest_table_file).is_dir():
            super(SparkModel, self).save(dest_table=dest_table)
            return

        deltaTable = DeltaTable.forPath(self.api, dest_table_file)
        merge_expr = ' AND '.join(f"dest.{key} = source.{key}" for key in primary_key)
        deltaTable.alias("dest") \
                  .merge(self._dataobject.alias("source"), merge_expr) \
                  .whenMatchedUpdateAll() \
                  .whenNotMatchedInsertAll() \
                  .execute()

    def saveAsTable(self, **kwargs):
        dest_table = kwargs.get('dest_table')
        mode = kwargs.get('mode') if kwargs.get('mode') is not None else 'overwrite'
        _, database_name, table_name = self.parse_dest_table(dest_table)


        self._dataobject.sql(f"CREATE DATABASE IF NOT EXISTS {database_name}")
        self._dataobject.sql(f"USE {database_name}")

        self._dataobject.write \
                        .format('delta') \
                        .mode(mode) \
                        .saveAsTable(table_name)

    def load_data(self, dest_table: str):
        zone_name, database_name, table_name = self.parse_dest_table(dest_table)

        return self.api.read \
                .format('delta') \
                .load(f"{self.data_path}/{zone_name}/{database_name}/{table_name}")


# Cell
@dataclass
class DataSource:
    #sqlserver or datalake
    source_type: str = 'sqlserver'
    #format: database.table
    source_table: str = None


@dataclass
class DestTable:
    name: str
    primary_key: List[str] = None

class PipelineCollection():
    "stores the list of funcs which will be called in AbstractModel compose"
    def __init__(self):
        self._funcs=defaultdict(dict)
        self._desttables={} # type: Dict[str, str]
        self._datasources=[] # type: List[DataSource]
        self._additional_data={} # type: Dict[str, List[str]]

    def register(self, *, pipeline_name: str = None, order : int = 0):
        "decorator to regist a function as a transform function, the functions will be called sequentially based on its order"
        if pipeline_name is None: raise Exception('Please provide pipleline name.')
        def decorate(func):
            func.order = order
            self._funcs[pipeline_name][func.__name__] = func
#             self._funcs[func.__name__]=func
            @wraps(func)
            def wrapper(*args, **kwargs):
                return func(*args, **kwargs)
            return wrapper
        return decorate

    def add_desttable(self, *, pipeline_name: str = None, desttable: str = None, primary_key: List[str] = None):
        if pipeline_name is None or desttable is None:
            raise Exception('Please provide pipeline name or desttable name')
        dest = DestTable(name=desttable, primary_key=primary_key)
        self._desttables[pipeline_name]=dest

    def get_pipeline_funcs(self, pipeline_name: str):
        return self._funcs.get(pipeline_name)

    def add_source(self, source: DataSource):
        self._datasources.append(source)

    def add_additional_data(self, pipeline_name: str = None, data_pipeline_names: List[str] = None):
        self._additional_data[pipeline_name] = data_pipeline_names

# Cell
class ModelMeta(type):
    "Meta class to add class attribute pipeline to all child classes of AbstractModel"
    @classmethod
    def __prepare__(cls, clsname, bases):
        return {'pipeline': PipelineCollection()}

class ModelAbcMeta(ModelMeta, abc.ABCMeta):
    pass

# Cell
PipelineFuncStruct = namedtuple('PipelineFuncStruct',['name','order','func'])

class AbstractModel(abc.ABC, metaclass=ModelAbcMeta):
#     pipeline = PipelineCollection()
    source_table = None
#     dest_table = None

    def __init__(self, *, reference=None, dataobject=None, api=None, data_path=None):
        self.reference = reference
        self._dataobject = dataobject
        self.api = api
        self.data_path = data_path
        self._pipeline_dataobjects = {}
        self.events = [] # type: List[events.Event]

    @classmethod
    def get_pipeline_names(cls):
        return list(cls.pipeline._funcs)

    @property
    def dataframe(self):
        return self._dataobject

    def __getattr__(self, name):
        if self._dataobject is None:
            raise Exception(f"{name} not implemented")

        return getattr(self._dataobject,name)

    def _copy(self):
        model=type(self).__new__(type(self))
        model.api=self.api
        model.data_path=self.data_path
        model.reference=self.reference
        model.events=copy.copy(self.events)
        return model

    def get_chained_pipelines(self, pipeline_name: str):
        chained_funcs = copy.copy(self.pipeline.get_pipeline_funcs(pipeline_name))
        for cls in type(self).__mro__:
            pipeline_collection = cls.__dict__.get('pipeline')
            if pipeline_collection is not None \
                and pipeline_collection.get_pipeline_funcs(pipeline_name) is not None \
                and isinstance(pipeline_collection, PipelineCollection):
                pipeline_funcs = pipeline_collection.get_pipeline_funcs(pipeline_name).copy()
                pipeline_funcs.update(chained_funcs)
                chained_funcs = pipeline_funcs
        return chained_funcs

    def sort_chained_pipelines(self, *, chained_funcs, order_key = 'order'):
        key = lambda o: getattr(o, order_key, 0)
        x = self._dataobject
        list_funcs = []
        for k,v in chained_funcs.items():
#             list_funcs.append({'name': v.__qualname__, '_order': v._order, 'func': v})
            list_funcs.append(PipelineFuncStruct(
                                name=v.__qualname__,
                                order=v.order,
                                func=v
                                )
                             )
        sorted_funcs = sorted(list_funcs, key=key)
        return sorted_funcs

    def print_pipeline(self, pipeline_name: str):
        chained_funcs = self.get_chained_pipelines(pipeline_name)
        sorted_funcs = self.sort_chained_pipelines(chained_funcs=chained_funcs)

        if len(sorted_funcs)<1:
            print("Pipeline name={pipeline_name} is empty")
            return
        pipeline_str = "Pipeline name = {}\nFunctions = {}(order={})" \
                            .format(pipeline_name, \
                                    sorted_funcs[0].name, \
                                    sorted_funcs[0].order \
                                   )
        for i, func in enumerate(sorted_funcs):
            if i>=1:
                pipeline_str+=" -> {}(order={})" \
                                .format(func.name, func.order)

        print(pipeline_str)

    def run_pipeline(self, *args, pipeline_name :str = None, order_key: str = 'order', **kwargs):
        '''traverse through the class hierarchy and
        call func in _funcs sequentially, return the result as a new AbstractModel object'''
#         db.set_trace()
        if pipeline_name is None: raise Exception('Please provide pipeline name.')
        if self.pipeline._funcs.get(pipeline_name) is None:
            raise Exception("Pipeline {pipeline_name} doesn't exist")

        chained_funcs = self.get_chained_pipelines(pipeline_name)

#         key = lambda o: getattr(o, order_key, 0)
#         x = self._dataobject
#         for f in sorted(commons.listify(chained_funcs), key=key):
#             x = f(self, x, *args, **kwargs)

        sorted_funcs = self.sort_chained_pipelines(chained_funcs=chained_funcs, order_key=order_key)

        x = self._dataobject
        pipeline_dataobjects = None



        if self.pipeline._additional_data.get(pipeline_name, None):
            pipeline_dataobjects = []
            for p_name in self.pipeline._additional_data.get(pipeline_name, None):
                o = self._pipeline_dataobjects.get(p_name, None)
                #db.set_trace()
                if not o:
                    raise Exception(f'Missing pipeline {p_name} data object. \
                                        Run the pipeline {p_name} to get the data object')
                pipeline_dataobjects.append(o)


        for f in sorted_funcs:
            if not pipeline_dataobjects:
                x = f.func(self, x, *args, **kwargs)
            else:
                x = f.func(self, x, *pipeline_dataobjects, *args, **kwargs)

        model = self._copy()
        model._dataobject = x
        self._pipeline_dataobjects[pipeline_name] = x
        return model

    def __repr__(self):
        class_name=type(self).__name__
        return '{0}({1._dataobject!r}, {1.pipeline!r})'.format(class_name, self)

    def __str__(self):
        return '({0._dataobject!s},{0.pipeline!s})'.format(self)


    def set_func_order(self, *, pipeline_name: str, func_name : str, order : int = 0 ):
        "change the order of the function in the _funcs dict for the object"
        func = self.pipeline._funcs[pipeline_name].get(func_name)
        if func is None:
            return
        logger.debug('set_func_order, old order is %s', func.order)
        func.__dict__['order'] = order
        logger.debug('set_func_order, new order is %s', func.order)

    def get_func_order(self, *, pipeline_name: str, func_name):
        func = self.pipeline._funcs[pipeline_name].get(func_name)
        return func.order

    @classmethod
    def from_dataobjects(cls, *args, reference:str, api:Any, data_path:str) -> AbstractModel:
        dataobject = cls._combine_dataobjects(*args)
        modelobj = cls( \
                reference=reference, \
                dataobject=dataobject,\
                api=api,\
                data_path=data_path
            )

        return modelobj

    @classmethod
    def _combine_dataobjects(cls, *args) -> Any:
        raise NotImplementedError

# Cell
class SparkModel(DeltaLakeMixin, AbstractModel):
    #format: database.table
    source_table = None
    #format: zone.database.table
#     dest_table = None
#     pipeline = PipelineCollection()

    def save(self, *, pipeline_name, mode :str ='overwrite'):
        #db.set_trace()
        dest_table = self.pipeline._desttables.get(pipeline_name, None)
        if not dest_table:
            return
        if not dest_table.primary_key:
            super().save(dest_table=dest_table.name, mode=mode)
        else:
            super().upsert(dest_table=dest_table.name, primary_key=dest_table.primary_key)

    def saveAsTable(self, *, pipeline_name, mode='overwrite'):
        dest_table = self.pipeline._desttables[pipeline_name]
        super().saveAsTable(dest_table=dest_table, mode=mode)

    def load(self, *, pipeline_name):
        dest_table = self.pipeline._desttables[pipeline_name]
        super().load(dest_table=dest_table)

# Cell
class PandasModel(AbstractModel):
    pass
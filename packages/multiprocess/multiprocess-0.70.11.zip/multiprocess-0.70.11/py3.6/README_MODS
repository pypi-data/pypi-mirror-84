cp -rf Python-3.6.0/Modules/_multiprocessing Modules/_multiprocessing
cp -rf Python-3.6.0/Lib/multiprocessing multiprocessing
cp -rf Python-3.6.0/Lib/test/*test_multiprocessing*.py tests/
cp -rf py3.5/examples .
cp -f py3.5/setup.py .
cp -rf py3.5/doc .
cp -f py3.5/index.html .

# ----------------------------------------------------------------------
diff Modules/_multiprocess/multiprocess.c Python-3.6.0/Modules/_multiprocessing/multiprocessing.c 
10c10
< #include "multiprocess.h"
---
> #include "multiprocessing.h"
144c144
<     "_multiprocess",
---
>     "_multiprocessing",
156c156
< PyInit__multiprocess(void)
---
> PyInit__multiprocessing(void)

diff Modules/_multiprocess/semaphore.c Python-3.6.0/Modules/_multiprocessing/semaphore.c      
10c10
< #include "multiprocess.h"
---
> #include "multiprocessing.h"
201c201
< // ifndef HAVE_SEM_TIMEDWAIT
---
> #ifndef HAVE_SEM_TIMEDWAIT
262c262
< // endif /* !HAVE_SEM_TIMEDWAIT */
---
> #endif /* !HAVE_SEM_TIMEDWAIT */
616c616
<     /* tp_name           */ "_multiprocess.SemLock",
---
>     /* tp_name           */ "_multiprocessing.SemLock",

diff multiprocess/__init__.py Python-3.6.0/Lib/multiprocessing/__init__.py
18,19d17
< __version__ = '0.70.5.dev0'
< 

diff multiprocess/connection.py Python-3.6.0/Lib/multiprocessing/connection.py
21,24c21
< try:
<     import _multiprocess as _multiprocessing
< except ImportError:
<     import _multiprocessing
---
> import _multiprocessing

diff multiprocess/forkserver.py Python-3.6.0/Lib/multiprocessing/forkserver.py
95c95
<             cmd = ('from multiprocess.forkserver import main; ' +
---
>             cmd = ('from multiprocessing.forkserver import main; ' +

diff multiprocess/popen_fork.py Python-3.6.0/Lib/multiprocessing/popen_fork.py
47c47
<                 from multiprocess.connection import wait
---
>                 from multiprocessing.connection import wait

diff multiprocess/popen_forkserver.py Python-3.6.0/Lib/multiprocessing/popen_forkserver.py
60c60
<             from multiprocess.connection import wait
---
>             from multiprocessing.connection import wait

diff multiprocess/queues.py Python-3.6.0/Lib/multiprocessing/queues.py
22,25c22
< try:
<     import _multiprocess as _multiprocessing
< except ImportError:
<     import _multiprocessing
---
> import _multiprocessing

diff multiprocess/reduction.py Python-3.6.0/Lib/multiprocessing/reduction.py
14,17c14
< try:
<     import dill as pickle
< except ImportError:
<     import pickle
---
> import pickle

diff multiprocess/semaphore_tracker.py Python-3.6.0/Lib/multiprocessing/semaphore_tracker.py
19,22c19
< try:
<     import _multiprocess as _multiprocessing
< except ImportError:
<     import _multiprocessing
---
> import _multiprocessing
53c50
<             cmd = 'from multiprocess.semaphore_tracker import main;main(%d)'
---
>             cmd = 'from multiprocessing.semaphore_tracker import main;main(%d)'

diff multiprocess/spawn.py Python-3.6.0/Lib/multiprocessing/spawn.py
89c86
<         prog = 'from multiprocess.spawn import spawn_main; spawn_main(%s)'
---
>         prog = 'from multiprocessing.spawn import spawn_main; spawn_main(%s)'

diff multiprocess/synchronize.py Python-3.6.0/Lib/multiprocessing/synchronize.py
17,20c17
< try:
<     import _multiprocess as _multiprocessing
< except ImportError:
<     import _multiprocessing
---
> import _multiprocessing
32,40c29,34
<     from _multiprocess import SemLock, sem_unlink
< except ImportError:
<     try:
<         from _multiprocessing import SemLock, sem_unlink
<     except (ImportError):
<         raise ImportError("This platform lacks a functioning sem_open" +
<                           " implementation, therefore, the required" +
<                           " synchronization primitives needed will not" +
<                           " function, see issue 3770.")
---
>     from _multiprocessing import SemLock, sem_unlink
> except (ImportError):
>     raise ImportError("This platform lacks a functioning sem_open" +
>                       " implementation, therefore, the required" +
>                       " synchronization primitives needed will not" +
>                       " function, see issue 3770.")

diff multiprocess/util.py Python-3.6.0/Lib/multiprocessing/util.py
38c38
< LOGGER_NAME = 'multiprocess'
---
> LOGGER_NAME = 'multiprocessing'

# ----------------------------------------------------------------------
REPLACED "from multiprocessing" with "from multiprocess"
REPLACED "from _multiprocessing" with "from _multiprocess"
REPLACED "import _multiprocessing" with "import _multiprocess as _multiprocessing"
REPLACED "multprocessing" with "multiprocess" wherever else relevant...
# ----------------------------------------------------------------------
diff Python-3.6.0/Lib/multiprocessing/context.py Python-3.6.6rc1/Lib/multiprocessing/context.py
199c199
<     def set_start_method(self, method=None):
---
>     def set_start_method(self, method, force=False):
Common subdirectories: Python-3.6.0/Lib/multiprocessing/dummy and Python-3.6.6rc1/Lib/multiprocessing/dummy
diff Python-3.6.0/Lib/multiprocessing/forkserver.py Python-3.6.6rc1/Lib/multiprocessing/forkserver.py
35a36
>         self._forkserver_pid = None
92,93c93,103
<             if self._forkserver_alive_fd is not None:
<                 return
---
>             if self._forkserver_pid is not None:
>                 # forkserver was launched before, is it still running?
>                 pid, status = os.waitpid(self._forkserver_pid, os.WNOHANG)
>                 if not pid:
>                     # still alive
>                     return
>                 # dead, launch it again
>                 os.close(self._forkserver_alive_fd)
>                 self._forkserver_address = None
>                 self._forkserver_alive_fd = None
>                 self._forkserver_pid = None
129a140
>                 self._forkserver_pid = pid
152,153c163,171
<     # ignoring SIGCHLD means no need to reap zombie processes
<     handler = signal.signal(signal.SIGCHLD, signal.SIG_IGN)
---
>     handlers = {
>         # no need to reap zombie processes;
>         signal.SIGCHLD: signal.SIG_IGN,
>         # protect the process from ^C
>         signal.SIGINT: signal.SIG_IGN,
>         }
>     old_handlers = {sig: signal.signal(sig, val)
>                     for (sig, val) in handlers.items()}
> 
178c196
<                             _serve_one(s, listener, alive_r, handler)
---
>                             _serve_one(s, listener, alive_r, old_handlers)
189,190c207,208
< def _serve_one(s, listener, alive_r, handler):
<     # close unnecessary stuff and reset SIGCHLD handler
---
> def _serve_one(s, listener, alive_r, handlers):
>     # close unnecessary stuff and reset signal handlers
193c211,212
<     signal.signal(signal.SIGCHLD, handler)
---
>     for sig, val in handlers.items():
>         signal.signal(sig, val)
diff Python-3.6.0/Lib/multiprocessing/pool.py Python-3.6.6rc1/Lib/multiprocessing/pool.py
121c121
<             if wrap_exception:
---
>             if wrap_exception and func is not _helper_reraises_exception:
130a131,132
> 
>         task = job = result = func = args = kwds = None
133a136,139
> def _helper_reraises_exception(ex):
>     'Pickle-able helper function for use by _guarded_task_generation.'
>     raise ex
> 
277a284,294
>     def _guarded_task_generation(self, result_job, func, iterable):
>         '''Provides a generator of tasks for imap and imap_unordered with
>         appropriate handling for iterables which throw exceptions during
>         iteration.'''
>         try:
>             i = -1
>             for i, x in enumerate(iterable):
>                 yield (result_job, i, func, (x,), {})
>         except Exception as e:
>             yield (result_job, i+1, _helper_reraises_exception, (e,), {})
> 
286,287c303,307
<             self._taskqueue.put((((result._job, i, func, (x,), {})
<                          for i, x in enumerate(iterable)), result._set_length))
---
>             self._taskqueue.put(
>                 (
>                     self._guarded_task_generation(result._job, func, iterable),
>                     result._set_length
>                 ))
293,294c313,319
<             self._taskqueue.put((((result._job, i, mapstar, (x,), {})
<                      for i, x in enumerate(task_batches)), result._set_length))
---
>             self._taskqueue.put(
>                 (
>                     self._guarded_task_generation(result._job,
>                                                   mapstar,
>                                                   task_batches),
>                     result._set_length
>                 ))
305,306c330,334
<             self._taskqueue.put((((result._job, i, func, (x,), {})
<                          for i, x in enumerate(iterable)), result._set_length))
---
>             self._taskqueue.put(
>                 (
>                     self._guarded_task_generation(result._job, func, iterable),
>                     result._set_length
>                 ))
312,313c340,346
<             self._taskqueue.put((((result._job, i, mapstar, (x,), {})
<                      for i, x in enumerate(task_batches)), result._set_length))
---
>             self._taskqueue.put(
>                 (
>                     self._guarded_task_generation(result._job,
>                                                   mapstar,
>                                                   task_batches),
>                     result._set_length
>                 ))
324c357
<         self._taskqueue.put(([(result._job, None, func, args, kwds)], None))
---
>         self._taskqueue.put(([(result._job, 0, func, args, kwds)], None))
355,356c388,395
<         self._taskqueue.put((((result._job, i, mapper, (x,), {})
<                               for i, x in enumerate(task_batches)), None))
---
>         self._taskqueue.put(
>             (
>                 self._guarded_task_generation(result._job,
>                                               mapper,
>                                               task_batches),
>                 None
>             )
>         )
378d416
<             i = -1
380c418,419
<                 for i, task in enumerate(taskseq):
---
>                 # iterating taskseq cannot fail
>                 for task in taskseq:
387c426
<                         job, ind = task[:2]
---
>                         job, idx = task[:2]
389c428
<                             cache[job]._set(ind, (False, e))
---
>                             cache[job]._set(idx, (False, e))
395c434,435
<                         set_length(i+1)
---
>                         idx = task[1] if task else -1
>                         set_length(idx + 1)
398,404c438,439
<             except Exception as ex:
<                 job, ind = task[:2] if task else (0, 0)
<                 if job in cache:
<                     cache[job]._set(ind + 1, (False, ex))
<                 if set_length:
<                     util.debug('doing set_length()')
<                     set_length(i+1)
---
>             finally:
>                 task = taskseq = job = None
408d442
< 
447a482
>             task = job = obj = None
463a499
>             task = job = obj = None
diff Python-3.6.0/Lib/multiprocessing/popen_fork.py Python-3.6.6rc1/Lib/multiprocessing/popen_fork.py
17,18c17
<         sys.stdout.flush()
<         sys.stderr.flush()
---
>         util._flush_std_streams()
diff Python-3.6.0/Lib/multiprocessing/process.py Python-3.6.6rc1/Lib/multiprocessing/process.py
106a107,109
>         # Avoid a refcycle if the target function holds an indirect
>         # reference to the process object (see bpo-30775)
>         del self._target, self._args, self._kwargs
131a135
> 
134,135c138,144
<         self._popen.poll()
<         return self._popen.returncode is None
---
> 
>         returncode = self._popen.poll()
>         if returncode is None:
>             return True
>         else:
>             _children.discard(self)
>             return False
268,269c277
<             sys.stdout.flush()
<             sys.stderr.flush()
---
>             util._flush_std_streams()
diff Python-3.6.0/Lib/multiprocessing/queues.py Python-3.6.6rc1/Lib/multiprocessing/queues.py
104c104
<                     if timeout < 0 or not self._poll(timeout):
---
>                     if not self._poll(timeout):
172,179c172
<         # On process exit we will wait for data to be flushed to pipe.
<         #
<         # However, if this process created the queue then all
<         # processes which use the queue will be descendants of this
<         # process.  Therefore waiting for the queue to be flushed
<         # is pointless once all the child processes have been joined.
<         created_by_this_process = (self._opid == os.getpid())
<         if not self._joincancelled and not created_by_this_process:
---
>         if not self._joincancelled:
224,225c217,218
<         try:
<             while 1:
---
>         while 1:
>             try:
252,259c245,251
<         except Exception as e:
<             if ignore_epipe and getattr(e, 'errno', 0) == errno.EPIPE:
<                 return
<             # Since this runs in a daemon thread the resources it uses
<             # may be become unusable while the process is cleaning up.
<             # We ignore errors which happen after the process has
<             # started to cleanup.
<             try:
---
>             except Exception as e:
>                 if ignore_epipe and getattr(e, 'errno', 0) == errno.EPIPE:
>                     return
>                 # Since this runs in a daemon thread the resources it uses
>                 # may be become unusable while the process is cleaning up.
>                 # We ignore errors which happen after the process has
>                 # started to cleanup.
261a254
>                     return
265,266d257
<             except Exception:
<                 pass
339a331
>         self._poll = self._reader.poll
diff Python-3.6.0/Lib/multiprocessing/semaphore_tracker.py Python-3.6.6rc1/Lib/multiprocessing/semaphore_tracker.py
31a32
>         self._pid = None
43,44c44,57
<             if self._fd is not None:
<                 return
---
>             if self._pid is not None:
>                 # semaphore tracker was launched before, is it still running?
>                 pid, status = os.waitpid(self._pid, os.WNOHANG)
>                 if not pid:
>                     # => still alive
>                     return
>                 # => dead, launch it again
>                 os.close(self._fd)
>                 self._fd = None
>                 self._pid = None
> 
>                 warnings.warn('semaphore_tracker: process died unexpectedly, '
>                               'relaunching.  Some semaphores might leak.')
> 
58c71
<                 util.spawnv_passfds(exe, args, fds_to_pass)
---
>                 pid = util.spawnv_passfds(exe, args, fds_to_pass)
63a77
>                 self._pid = pid
diff Python-3.6.0/Lib/multiprocessing/spawn.py Python-3.6.6rc1/Lib/multiprocessing/spawn.py
220c220
<         set_start_method(data['start_method'])
---
>         set_start_method(data['start_method'], force=True)
diff Python-3.6.0/Lib/multiprocessing/util.py Python-3.6.6rc1/Lib/multiprocessing/util.py
244c244
<         f = lambda p : p[0][0] is not None
---
>         f = lambda p : p[0] is not None
246c246
<         f = lambda p : p[0][0] is not None and p[0][0] >= minpriority
---
>         f = lambda p : p[0] is not None and p[0] >= minpriority
248,249c248,249
<     items = [x for x in list(_finalizer_registry.items()) if f(x)]
<     items.sort(reverse=True)
---
>     # Careful: _finalizer_registry may be mutated while this function
>     # is running (either by a GC run or by another thread).
251,257c251,265
<     for key, finalizer in items:
<         sub_debug('calling %s', finalizer)
<         try:
<             finalizer()
<         except Exception:
<             import traceback
<             traceback.print_exc()
---
>     # list(_finalizer_registry) should be atomic, while
>     # list(_finalizer_registry.items()) is not.
>     keys = [key for key in list(_finalizer_registry) if f(key)]
>     keys.sort(reverse=True)
> 
>     for key in keys:
>         finalizer = _finalizer_registry.get(key)
>         # key may have been removed from the registry
>         if finalizer is not None:
>             sub_debug('calling %s', finalizer)
>             try:
>                 finalizer()
>             except Exception:
>                 import traceback
>                 traceback.print_exc()
383a392,405
> # Flush standard streams, if any
> #
> 
> def _flush_std_streams():
>     try:
>         sys.stdout.flush()
>     except (AttributeError, ValueError):
>         pass
>     try:
>         sys.stderr.flush()
>     except (AttributeError, ValueError):
>         pass
> 
> #
389c411
<     passfds = sorted(passfds)
---
>     passfds = tuple(sorted(map(int, passfds)))
# ----------------------------------------------------------------------
diff Python-3.6.6rc1/Modules/_multiprocessing/semaphore.c Python-3.6.8/Modules/_multiprocessing/semaphore.c
442,443c442,444
<         if (name_copy == NULL)
<             goto failure;
---
>         if (name_copy == NULL) {
>             return PyErr_NoMemory();
>         }
466c467,469
<     _PyMp_SetError(NULL, MP_STANDARD_ERROR);
---
>     if (!PyErr_Occurred()) {
>         _PyMp_SetError(NULL, MP_STANDARD_ERROR);
>     }
diff Python-3.6.6rc1/Lib/multiprocessing/connection.py Python-3.6.8/Lib/multiprocessing/connection.py
60c60
<     return time.time() + timeout
---
>     return time.monotonic() + timeout
63c63
<     return time.time() > t
---
>     return time.monotonic() > t
908c908
<                 deadline = time.time() + timeout
---
>                 deadline = time.monotonic() + timeout
916c916
<                         timeout = deadline - time.time()
---
>                         timeout = deadline - time.monotonic()
Common subdirectories: Python-3.6.6rc1/Lib/multiprocessing/dummy and Python-3.6.8/Lib/multiprocessing/dummy
diff Python-3.6.6rc1/Lib/multiprocessing/managers.py Python-3.6.8/Lib/multiprocessing/managers.py
20a21
> import time
22d22
< from time import time as _time
1009c1009
<             endtime = _time() + timeout
---
>             endtime = time.monotonic() + timeout
1015c1015
<                 waittime = endtime - _time()
---
>                 waittime = endtime - time.monotonic()
1098c1098
<     '__contains__', '__delitem__', '__getitem__', '__len__',
---
>     '__contains__', '__delitem__', '__getitem__', '__iter__', '__len__',
1101a1102,1104
> DictProxy._method_to_typeid_ = {
>     '__iter__': 'Iterator',
>     }
diff Python-3.6.6rc1/Lib/multiprocessing/queues.py Python-3.6.8/Lib/multiprocessing/queues.py
98c98
<                 deadline = time.time() + timeout
---
>                 deadline = time.monotonic() + timeout
103c103
<                     timeout = deadline - time.time()
---
>                     timeout = deadline - time.monotonic()
diff Python-3.6.6rc1/Lib/multiprocessing/reduction.py Python-3.6.8/Lib/multiprocessing/reduction.py
153c153
<         msg, ancdata, flags, addr = sock.recvmsg(1, socket.CMSG_LEN(bytes_size))
---
>         msg, ancdata, flags, addr = sock.recvmsg(1, socket.CMSG_SPACE(bytes_size))
diff Python-3.6.6rc1/Lib/multiprocessing/synchronize.py Python-3.6.8/Lib/multiprocessing/synchronize.py
18,19c18
< 
< from time import time as _time
---
> import time
316c315
<             endtime = _time() + timeout
---
>             endtime = time.monotonic() + timeout
322c321
<                 waittime = endtime - _time()
---
>                 waittime = endtime - time.monotonic()
diff Python-3.6.6rc1/Lib/test/test_multiprocessing_fork.py Python-3.6.8/Lib/test/test_multiprocessing_fork.py 
3a4
> import sys
8a10,14
> if sys.platform == "win32":
>     raise unittest.SkipTest("fork is not available on Windows")
> 
> if sys.platform == 'darwin':
>     raise unittest.SkipTest("test may crash on macOS (bpo-33725)")
diff Python-3.6.6rc1/Lib/test/test_multiprocessing_forkserver.py Python-3.6.8/Lib/test/test_multiprocessing_forkserver.py 
3a4
> import sys
8a10,12
> if sys.platform == "win32":
>     raise unittest.SkipTest("forkserver is not available on Windows")
> 
diff Python-3.6.6rc1/Lib/test/test_multiprocessing_main_handling.py Python-3.6.8/Lib/test/test_multiprocessing_main_handling.py 
61c61
<     deadline = time.time() + 10 # up to 10 s to report the results
---
>     start_time = time.monotonic()
64,65c64,67
<         if time.time() > deadline:
<             raise RuntimeError("Timed out waiting for results")
---
>         # up to 1 min to report the results
>         dt = time.monotonic() - start_time
>         if dt > 60.0:
>             raise RuntimeError("Timed out waiting for results (%.1f sec)" % dt)
89c91
< deadline = time.time() + 10 # up to 10 s to report the results
---
> start_time = time.monotonic()
92,93c94,97
<     if time.time() > deadline:
<         raise RuntimeError("Timed out waiting for results")
---
>     # up to 1 min to report the results
>     dt = time.monotonic() - start_time
>     if dt > 60.0:
>         raise RuntimeError("Timed out waiting for results (%.1f sec)" % dt)
# ----------------------------------------------------------------------
ADDED *args, **kwds for ForkingPickler in __init__, dump, and dumps

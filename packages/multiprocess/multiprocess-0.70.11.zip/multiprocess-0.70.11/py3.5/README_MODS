cp -rf Python-3.5.0/Modules/_multiprocessing Modules/_multiprocessing
cp -rf Python-3.5.0/Lib/multiprocessing multiprocessing
cp -rf Python-3.5.0/Lib/test/*test_multiprocessing*.py tests/
cp -rf py3.4.3/examples .
cp -f py3.4.3/*txt .
cp -f py3.4.3/setup.py .
cp -rf py3.4.3/doc .
cp -f py3.4.3/index.html .

# ----------------------------------------------------------------------
diff Modules/_multiprocessing/semaphore.c Python-3.5.0/Modules/_multiprocessing/semaphore.c
201c201
< //ifndef HAVE_SEM_TIMEDWAIT
---
> #ifndef HAVE_SEM_TIMEDWAIT
262c262
< //endif /* !HAVE_SEM_TIMEDWAIT */
---
> #endif /* !HAVE_SEM_TIMEDWAIT */

# ----------------------------------------------------------------------
diff multiprocessing/__init__.py Python-3.5.0/Lib/multiprocessing/__init__.py
18,19d17
< __version__ = '0.70.1'
< 

diff multiprocessing/reduction.py Python-3.5.0/Lib/multiprocessing/reduction.py
14,17c14
< try:
<     import dill as pickle
< except ImportError:
<     import pickle
---
> import pickle

diff multiprocessing/spawn.py Python-3.5.0/Lib/multiprocessing/spawn.py
12,15c12
< try:
<     import dill as pickle
< except ImportError:
<     import pickle
---
> import pickle

# ----------------------------------------------------------------------
REPLACED "from multiprocessing" with "from multiprocess"
REPLACED "from _multiprocessing" with "from _multiprocess"
REPLACED "import _multiprocessing" with "import _multiprocess as _multiprocessing"
REPLACED "multprocessing" with "multiprocess" wherever else relevant...

# ----------------------------------------------------------------------
diff Python-3.5.0/Lib/multiprocessing/connection.py Python-3.5.2/Lib/multiprocessing/connection.py
400c400
<             # Issue #<E2><80><AF>20540: concatenate before sending, to avoid delays due
---
>             # Issue #20540: concatenate before sending, to avoid delays due
Common subdirectories: Python-3.5.0/Lib/multiprocessing/dummy and Python-3.5.2/Lib/multiprocessing/dummy

diff Python-3.5.0/Lib/multiprocessing/forkserver.py Python-3.5.2/Lib/multiprocessing/forkserver.py
150,156c150
<     # close sys.stdin
<     if sys.stdin is not None:
<         try:
<             sys.stdin.close()
<             sys.stdin = open(os.devnull)
<         except (OSError, ValueError):
<             pass
---
>     util._close_stdin()

diff Python-3.5.0/Lib/multiprocessing/managers.py Python-3.5.2/Lib/multiprocessing/managers.py
845c845
<     Return an proxy type whose methods are given by `exposed`
---
>     Return a proxy type whose methods are given by `exposed`

diff Python-3.5.0/Lib/multiprocessing/process.py Python-3.5.2/Lib/multiprocessing/process.py
237,242c237
<             if sys.stdin is not None:
<                 try:
<                     sys.stdin.close()
<                     sys.stdin = open(os.devnull)
<                 except (OSError, ValueError):
<                     pass
---
>             util._close_stdin()

diff Python-3.5.0/Lib/multiprocessing/spawn.py Python-3.5.2/Lib/multiprocessing/spawn.py
94c94
<     Run code specifed by data received over pipe
---
>     Run code specified by data received over pipe

diff Python-3.5.0/Lib/multiprocessing/util.py Python-3.5.2/Lib/multiprocessing/util.py
11a12
> import sys
358a360,381
> #
> # Close sys.stdin and replace stdin with os.devnull
> #
> 
> def _close_stdin():
>     if sys.stdin is None:
>         return
> 
>     try:
>         sys.stdin.close()
>     except (OSError, ValueError):
>         pass
> 
>     try:
>         fd = os.open(os.devnull, os.O_RDONLY)
>         try:
>             sys.stdin = open(fd, closefd=False)
>         except:
>             os.close(fd)
>             raise
>     except (OSError, ValueError):
>         pass

# ----------------------------------------------------------------------
diff Python-3.5.2/Modules/_multiprocessing/semaphore.c Python-3.5.5/Modules/_multiprocessing/semaphore.c
116a117,119
>     else {
>         sigint_event = NULL;
>     }
diff Python-3.5.2/Lib/multiprocessing/context.py Python-3.5.5/Lib/multiprocessing/context.py
198c198
<     def set_start_method(self, method=None):
---
>     def set_start_method(self, method, force=False):
Common subdirectories: Python-3.5.2/Lib/multiprocessing/dummy and Python-3.5.5/Lib/multiprocessing/dummy
diff Python-3.5.2/Lib/multiprocessing/forkserver.py Python-3.5.5/Lib/multiprocessing/forkserver.py
152,153c152,160
<     # ignoring SIGCHLD means no need to reap zombie processes
<     handler = signal.signal(signal.SIGCHLD, signal.SIG_IGN)
---
>     # ignoring SIGCHLD means no need to reap zombie processes;
>     # letting SIGINT through avoids KeyboardInterrupt tracebacks
>     handlers = {
>         signal.SIGCHLD: signal.SIG_IGN,
>         signal.SIGINT: signal.SIG_DFL,
>         }
>     old_handlers = {sig: signal.signal(sig, val)
>                     for (sig, val) in handlers.items()}
> 
178c185
<                             _serve_one(s, listener, alive_r, handler)
---
>                             _serve_one(s, listener, alive_r, old_handlers)
189,190c196,197
< def _serve_one(s, listener, alive_r, handler):
<     # close unnecessary stuff and reset SIGCHLD handler
---
> def _serve_one(s, listener, alive_r, handlers):
>     # close unnecessary stuff and reset signal handlers
193c200,201
<     signal.signal(signal.SIGCHLD, handler)
---
>     for sig, val in handlers.items():
>         signal.signal(sig, val)
diff Python-3.5.2/Lib/multiprocessing/managers.py Python-3.5.5/Lib/multiprocessing/managers.py
278c278
<                     send(('#UNSERIALIZABLE', repr(msg)))
---
>                     send(('#UNSERIALIZABLE', format_exc()))
diff Python-3.5.2/Lib/multiprocessing/pool.py Python-3.5.5/Lib/multiprocessing/pool.py
121c121
<             if wrap_exception:
---
>             if wrap_exception and func is not _helper_reraises_exception:
130a131,132
> 
>         task = job = result = func = args = kwds = None
133a136,139
> def _helper_reraises_exception(ex):
>     'Pickle-able helper function for use by _guarded_task_generation.'
>     raise ex
> 
277a284,294
>     def _guarded_task_generation(self, result_job, func, iterable):
>         '''Provides a generator of tasks for imap and imap_unordered with
>         appropriate handling for iterables which throw exceptions during
>         iteration.'''
>         try:
>             i = -1
>             for i, x in enumerate(iterable):
>                 yield (result_job, i, func, (x,), {})
>         except Exception as e:
>             yield (result_job, i+1, _helper_reraises_exception, (e,), {})
> 
286,287c303,307
<             self._taskqueue.put((((result._job, i, func, (x,), {})
<                          for i, x in enumerate(iterable)), result._set_length))
---
>             self._taskqueue.put(
>                 (
>                     self._guarded_task_generation(result._job, func, iterable),
>                     result._set_length
>                 ))
293,294c313,319
<             self._taskqueue.put((((result._job, i, mapstar, (x,), {})
<                      for i, x in enumerate(task_batches)), result._set_length))
---
>             self._taskqueue.put(
>                 (
>                     self._guarded_task_generation(result._job,
>                                                   mapstar,
>                                                   task_batches),
>                     result._set_length
>                 ))
305,306c330,334
<             self._taskqueue.put((((result._job, i, func, (x,), {})
<                          for i, x in enumerate(iterable)), result._set_length))
---
>             self._taskqueue.put(
>                 (
>                     self._guarded_task_generation(result._job, func, iterable),
>                     result._set_length
>                 ))
312,313c340,346
<             self._taskqueue.put((((result._job, i, mapstar, (x,), {})
<                      for i, x in enumerate(task_batches)), result._set_length))
---
>             self._taskqueue.put(
>                 (
>                     self._guarded_task_generation(result._job,
>                                                   mapstar,
>                                                   task_batches),
>                     result._set_length
>                 ))
324c357
<         self._taskqueue.put(([(result._job, None, func, args, kwds)], None))
---
>         self._taskqueue.put(([(result._job, 0, func, args, kwds)], None))
355,356c388,395
<         self._taskqueue.put((((result._job, i, mapper, (x,), {})
<                               for i, x in enumerate(task_batches)), None))
---
>         self._taskqueue.put(
>             (
>                 self._guarded_task_generation(result._job,
>                                               mapper,
>                                               task_batches),
>                 None
>             )
>         )
378d416
<             i = -1
380c418,419
<                 for i, task in enumerate(taskseq):
---
>                 # iterating taskseq cannot fail
>                 for task in taskseq:
387c426
<                         job, ind = task[:2]
---
>                         job, idx = task[:2]
389c428
<                             cache[job]._set(ind, (False, e))
---
>                             cache[job]._set(idx, (False, e))
395c434,435
<                         set_length(i+1)
---
>                         idx = task[1] if task else -1
>                         set_length(idx + 1)
398,404c438,439
<             except Exception as ex:
<                 job, ind = task[:2] if task else (0, 0)
<                 if job in cache:
<                     cache[job]._set(ind + 1, (False, ex))
<                 if set_length:
<                     util.debug('doing set_length()')
<                     set_length(i+1)
---
>             finally:
>                 task = taskseq = job = None
408d442
< 
447a482
>             task = job = obj = None
463a499
>             task = job = obj = None
diff Python-3.5.2/Lib/multiprocessing/process.py Python-3.5.5/Lib/multiprocessing/process.py
106a107,109
>         # Avoid a refcycle if the target function holds an indirect
>         # reference to the process object (see bpo-30775)
>         del self._target, self._args, self._kwargs
diff Python-3.5.2/Lib/multiprocessing/queues.py Python-3.5.5/Lib/multiprocessing/queues.py
172,179c172
<         # On process exit we will wait for data to be flushed to pipe.
<         #
<         # However, if this process created the queue then all
<         # processes which use the queue will be descendants of this
<         # process.  Therefore waiting for the queue to be flushed
<         # is pointless once all the child processes have been joined.
<         created_by_this_process = (self._opid == os.getpid())
<         if not self._joincancelled and not created_by_this_process:
---
>         if not self._joincancelled:
224,225c217,218
<         try:
<             while 1:
---
>         while 1:
>             try:
252,259c245,251
<         except Exception as e:
<             if ignore_epipe and getattr(e, 'errno', 0) == errno.EPIPE:
<                 return
<             # Since this runs in a daemon thread the resources it uses
<             # may be become unusable while the process is cleaning up.
<             # We ignore errors which happen after the process has
<             # started to cleanup.
<             try:
---
>             except Exception as e:
>                 if ignore_epipe and getattr(e, 'errno', 0) == errno.EPIPE:
>                     return
>                 # Since this runs in a daemon thread the resources it uses
>                 # may be become unusable while the process is cleaning up.
>                 # We ignore errors which happen after the process has
>                 # started to cleanup.
261a254
>                     return
265,266d257
<             except Exception:
<                 pass
339a331
>         self._poll = self._reader.poll
diff Python-3.5.2/Lib/multiprocessing/spawn.py Python-3.5.5/Lib/multiprocessing/spawn.py
221c221
<         set_start_method(data['start_method'])
---
>         set_start_method(data['start_method'], force=True)
diff Python-3.5.2/Lib/multiprocessing/util.py Python-3.5.5/Lib/multiprocessing/util.py
244c244
<         f = lambda p : p[0][0] is not None
---
>         f = lambda p : p[0] is not None
246c246
<         f = lambda p : p[0][0] is not None and p[0][0] >= minpriority
---
>         f = lambda p : p[0] is not None and p[0] >= minpriority
248,249c248,249
<     items = [x for x in list(_finalizer_registry.items()) if f(x)]
<     items.sort(reverse=True)
---
>     # Careful: _finalizer_registry may be mutated while this function
>     # is running (either by a GC run or by another thread).
251,257c251,265
<     for key, finalizer in items:
<         sub_debug('calling %s', finalizer)
<         try:
<             finalizer()
<         except Exception:
<             import traceback
<             traceback.print_exc()
---
>     # list(_finalizer_registry) should be atomic, while
>     # list(_finalizer_registry.items()) is not.
>     keys = [key for key in list(_finalizer_registry) if f(key)]
>     keys.sort(reverse=True)
> 
>     for key in keys:
>         finalizer = _finalizer_registry.get(key)
>         # key may have been removed from the registry
>         if finalizer is not None:
>             sub_debug('calling %s', finalizer)
>             try:
>                 finalizer()
>             except Exception:
>                 import traceback
>                 traceback.print_exc()
389c397
<     passfds = sorted(passfds)
---
>     passfds = tuple(sorted(map(int, passfds)))
# ----------------------------------------------------------------------
ADDED *args, **kwds for ForkingPickler in __init__, dump, and dumps
